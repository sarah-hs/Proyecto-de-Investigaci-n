{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from itertools import groupby\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "#from coremltools.converters.keras import convert\n",
    "#from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT, IMG_WIDTH = (224, 224)\n",
    "img_folder = Path(\"./images/101_ObjectCategories/\").expanduser()\n",
    "coreml_model_file = \"similarity.mlmodel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Network for Feature Extraction\n",
    "We load the model which has been trained on ImageNet. We specify `include_top=False`.  This ensures that we don't load the final layers specific to the classes the model was originall trained to predict. For more information, see the [Keras documentation](https://keras.io/applications/#resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 603s 6us/step\n"
     ]
    }
   ],
   "source": [
    "encoder_model = ResNet50(input_shape=(IMG_HEIGHT,IMG_WIDTH,3), weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(encoder_model.output.shape.as_list()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test images:  0\n",
      "train images 0\n"
     ]
    }
   ],
   "source": [
    "image_filenames = glob.glob(str(img_folder / '**/*.jpg'))\n",
    "\n",
    "test_filenames = []\n",
    "train_filenames = []\n",
    "for key, items in groupby(sorted(image_filenames), lambda f: Path(f).parts[-2]):\n",
    "    if key == 'BACKGROUND_Google':\n",
    "        continue\n",
    "    test, *train = items\n",
    "    test_filenames.append(test)\n",
    "    train_filenames += train\n",
    "\n",
    "print(\"test images: \", len(test_filenames))\n",
    "print(\"train images\", len(train_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features for all images in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-337b833c8028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mencoded_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_encode_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-337b833c8028>\u001b[0m in \u001b[0;36mload_encode_images\u001b[0;34m(encoder, filenames)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_encode_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mencoded_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfile_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "def load_encode_images(encoder, filenames):\n",
    "    batch_size = 16\n",
    "    encoded_dim = np.prod(encoder.output.shape[1:]).value\n",
    "    file_count = len(filenames)\n",
    "    encoded = np.zeros((file_count, encoded_dim))\n",
    "    for start_index in tqdm(list(range(0, file_count, batch_size))):\n",
    "        end_index = min(start_index + batch_size, file_count)\n",
    "        batch_filenames = filenames[start_index:end_index]\n",
    "\n",
    "        batch_images = load_images(batch_filenames)\n",
    "        batch_encoded = encoder.predict(batch_images)\n",
    "        batch_encoded_flat = batch_encoded.reshape(len(batch_images), -1)\n",
    "        encoded[start_index:end_index, :] = batch_encoded_flat\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def load_images(filenames):\n",
    "    images = np.zeros((len(filenames), IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    for i, filename in enumerate(filenames):\n",
    "        img = image.load_img(filename, target_size=(IMG_HEIGHT,IMG_WIDTH))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        images[i, :, :, :] = img_array\n",
    "    return images\n",
    "\n",
    "encoded_imgs = load_encode_images(encoder_model, train_filenames).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the k-NN and Joined Model\n",
    "Here we build the actual k-NN and combine it with the existing network. All there is to the K-NN is the Dense layer. We do not need a bias so we specify `use_bias=False`. It's also important to note that we don't need an activation function, hence `actication='linear'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knn(model, output_size):\n",
    "    # Flatten feature vector\n",
    "    flat_dim_size = np.prod(model.output_shape[1:])\n",
    "    x = Reshape(target_shape=(flat_dim_size,),\n",
    "                name='features_flat')(model.output)\n",
    "    \n",
    "    # Dot product between feature vector and reference vectors\n",
    "    x = Dense(units=output_size,\n",
    "              activation='linear',\n",
    "              name='dense_1',\n",
    "              use_bias=False)(x)   \n",
    "                \n",
    "    classifier = Model(inputs=[model.input], outputs=x)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_model = build_knn(encoder_model, encoded_imgs.shape[1])\n",
    "joined_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ecnodings(encodings):\n",
    "    ref_norms = np.linalg.norm(encoded_imgs, axis=0)\n",
    "    return encodings / ref_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs_normalized = normalize_ecnodings(encoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Weights to Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weights = joined_model.get_weights()\n",
    "temp_weights[-1] = encoded_imgs_normalized\n",
    "joined_model.set_weights(temp_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_filename = test_filenames[0]\n",
    "print(example_filename)\n",
    "example_img = image.load_img(example_filename, target_size=(IMG_WIDTH, IMG_HEIGHT))\n",
    "example_img = image.img_to_array(example_img)\n",
    "example_img = np.expand_dims(example_img, axis=0)\n",
    "example_img = preprocess_input(example_img)\n",
    "prediction = joined_model.predict([example_img]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in prediction.argsort()[-5:][::-1]:\n",
    "    print(train_filenames[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CoreML\n",
    "We now conver to CoreML. Note that we specify `is_bgr=True`. This is because the weights in the ResNet50 model are learned with Caffe which uses BGR as opposed to RGB. We also set a bias for each color channel. This is to simulate the zero-centering that Keras would normally take care of in the `preprocess_input` function (which we use above when loading the images). From the documentation for that function:\n",
    "\n",
    "> will convert the images from RGB to BGR,\n",
    "> then will zero-center each color channel with\n",
    "> respect to the ImageNet dataset,\n",
    "> without scaling.\n",
    "\n",
    "The actual mean values used for centering can be found in the Keras Applications [source code](https://github.com/keras-team/keras-applications/blob/2661dac4dacb717e54640f158cfa9bacae6dd91b/keras_applications/imagenet_utils.py#L64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_to_coreml(joined_model, coreml_model_file):\n",
    "    coreml_encoder = convert(joined_model,\n",
    "                             input_names=['encoder_input'],\n",
    "                             image_input_names=['encoder_input'],\n",
    "                             output_names=['dense_1'],\n",
    "                             is_bgr=True,\n",
    "                             red_bias=-123.68,\n",
    "                             green_bias=-116.779,\n",
    "                             blue_bias=-103.939)\n",
    "    coreml_encoder.author = 'Soren Lind Kristiansen'\n",
    "    coreml_encoder.license = 'N/A'\n",
    "    coreml_encoder.short_description = 'Image similarity.'\n",
    "    coreml_encoder.output_description['dense_1'] = 'k-NN'\n",
    "    coreml_encoder.save(coreml_model_file)\n",
    "\n",
    "keras_to_coreml(joined_model, coreml_model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PI",
   "language": "python",
   "name": "pi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
