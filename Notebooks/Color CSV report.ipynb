{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sarah/opt/miniconda3/envs/PI/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "CSV_FILE = \"./Resources/LomaLinda.csv\"\n",
    "classes_dict = {0:'Diorite', 1:'Gabbro', 2:'Granite', 3:'Granodiorite'}\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (283, 16) y shape: (283,)\n",
      "Classes: [0. 1. 2. 3.]\n",
      "Counts: [78 65 70 70]\n",
      "Indices: [140 218  70   0]\n",
      "Classes dictionary: {0: 'Diorite', 1: 'Gabbro', 2: 'Granite', 3: 'Granodiorite'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt(CSV_FILE, delimiter=\",\")\n",
    "\n",
    "#data = data[data[:,-1] != 0]\n",
    "#data = data[data[:,-1] != 1]\n",
    "#data = data[data[:,-1] != 2]\n",
    "#data = data[data[:,-1] != 3]\n",
    "\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "print('x shape:', x.shape, 'y shape:', y.shape)\n",
    "\n",
    "classes, indices, counts = np.unique(y, return_counts=True, return_index=True)\n",
    "\n",
    "print('Classes:', classes)\n",
    "print('Counts:', counts)\n",
    "print('Indices:', indices)\n",
    "print('Classes dictionary:', classes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SPLIT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val = 20%, test = 16%, train = 64%\n",
    "from sklearn.model_selection import train_test_split as splitter\n",
    "xtt, xval, ytt, yval = splitter(x,y,train_size=0.8,random_state=42)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = splitter(xtt,ytt,train_size=0.8,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniques(arr, name):\n",
    "    print('<<', name, '>>')\n",
    "    classes, indices, counts = np.unique(arr, return_counts=True, return_index=True)\n",
    "    print('Classes:', classes)\n",
    "    print('Counts:', counts)\n",
    "    print('Indices:', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< Train >>\n",
      "Classes: [0. 1. 2. 3.]\n",
      "Counts: [50 40 45 45]\n",
      "Indices: [4 0 2 1]\n",
      "<< Test >>\n",
      "Classes: [0. 1. 2. 3.]\n",
      "Counts: [11 12 12 11]\n",
      "Indices: [2 3 1 0]\n",
      "<< Val >>\n",
      "Classes: [0. 1. 2. 3.]\n",
      "Counts: [17 13 13 14]\n",
      "Indices: [3 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "get_uniques(ytrain, 'Train')\n",
    "get_uniques(ytest, 'Test')\n",
    "get_uniques(yval, 'Val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. NORMALIZE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "xtrainNorm = normalizer.fit_transform(xtrain)\n",
    "xtestNorm = normalizer.fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logReg = LogisticRegression(solver='lbfgs')\n",
    "logReg.fit(xtrainNorm, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(xtrainNorm, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 Prediction: 0 True Class: 0\n",
      "Test 1 Prediction: 0 True Class: 0\n",
      "Test 2 Prediction: 0 True Class: 0\n",
      "Test 3 Prediction: 0 True Class: 0\n",
      "Test 4 Prediction: 0 True Class: 0\n",
      "Test 5 Prediction: 0 True Class: 0\n",
      "Test 6 Prediction: 0 True Class: 0\n",
      "Test 7 Prediction: 0 True Class: 0\n",
      "Test 8 Prediction: 0 True Class: 0\n",
      "Test 9 Prediction: 0 True Class: 0\n",
      "Test 10 Prediction: 0 True Class: 0\n",
      "Test 11 Prediction: 0 True Class: 0\n",
      "Test 12 Prediction: 0 True Class: 0\n",
      "Test 13 Prediction: 0 True Class: 0\n",
      "Test 14 Prediction: 0 True Class: 0\n",
      "Test 15 Prediction: 0 True Class: 0\n",
      "Test 16 Prediction: 0 True Class: 0\n",
      "Test 17 Prediction: 0 True Class: 0\n",
      "Test 18 Prediction: 0 True Class: 0\n",
      "Test 19 Prediction: 0 True Class: 0\n",
      "Test 20 Prediction: 0 True Class: 0\n",
      "Test 21 Prediction: 0 True Class: 0\n",
      "Test 22 Prediction: 0 True Class: 0\n",
      "Test 23 Prediction: 0 True Class: 0\n",
      "Test 24 Prediction: 0 True Class: 0\n",
      "Test 25 Prediction: 0 True Class: 0\n",
      "Test 26 Prediction: 0 True Class: 0\n",
      "Test 27 Prediction: 0 True Class: 0\n",
      "Test 28 Prediction: 0 True Class: 0\n",
      "Test 29 Prediction: 0 True Class: 0\n",
      "Test 30 Prediction: 0 True Class: 0\n",
      "Test 31 Prediction: 0 True Class: 0\n",
      "Test 32 Prediction: 0 True Class: 0\n",
      "Test 33 Prediction: 0 True Class: 0\n",
      "Test 34 Prediction: 0 True Class: 0\n",
      "Test 35 Prediction: 0 True Class: 0\n",
      "Test 36 Prediction: 0 True Class: 0\n",
      "Test 37 Prediction: 0 True Class: 0\n",
      "Test 38 Prediction: 0 True Class: 0\n",
      "Test 39 Prediction: 0 True Class: 0\n",
      "Test 40 Prediction: 0 True Class: 0\n",
      "Test 41 Prediction: 0 True Class: 0\n",
      "Test 42 Prediction: 0 True Class: 0\n",
      "Test 43 Prediction: 0 True Class: 0\n",
      "Test 44 Prediction: 0 True Class: 0\n",
      "Test 45 Prediction: 0 True Class: 0\n",
      "Accuracy :  0.9999999999999994\n",
      "INFO:tensorflow:Froze 0 variables.\n",
      "INFO:tensorflow:Converted 0 variables to const ops.\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "See console for info.\n/bin/sh: toco_from_protos: command not found\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-86054a1e51f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Convert the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxtr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Save the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/PI/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m           \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m           \u001b[0moutput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m           **converter_kwargs)\n\u001b[0m\u001b[1;32m    984\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m       result = _toco_convert_graph_def(\n",
      "\u001b[0;32m~/opt/miniconda3/envs/PI/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[0;34m(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m       \u001b[0mdebug_info_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug_info_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m       enable_mlir_converter=enable_mlir_converter)\n\u001b[0m\u001b[1;32m    450\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/PI/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConverterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"See console for info.\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# Must manually cleanup files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConverterError\u001b[0m: See console for info.\n/bin/sh: toco_from_protos: command not found\n\n\n"
     ]
    }
   ],
   "source": [
    "# tf Graph Input\n",
    "xtr = tf.placeholder(\"float\", [180, 16])\n",
    "xte = tf.placeholder(\"float\", [16])\n",
    "\n",
    "var = tf.get_variable('weights', dtype=tf.float32, shape=(180,16))\n",
    "val = xtr + var\n",
    "out = tf.identity(val, name='out')\n",
    "\n",
    "# Nearest Neighbor calculation using L1 Distance\n",
    "# Calculate L1 Distance\n",
    "distance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=1)\n",
    "# Prediction: Get min distance index (Nearest neighbor)\n",
    "pred = tf.arg_min(distance, 0)\n",
    "\n",
    "accuracy = 0.\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # loop over test data\n",
    "    for i in range(len(xtestNorm)):\n",
    "        # Get nearest neighbor\n",
    "        nn_index = sess.run(pred, feed_dict={xtr: xtrainNorm, xte: xtestNorm[i, :]})\n",
    "        # Get nearest neighbor class label and compare it to its true label\n",
    "        print(\"Test\", i, \"Prediction:\", np.argmax(ytrain[nn_index]), \\\n",
    "            \"True Class:\", np.argmax(ytest[i]))\n",
    "        # Calculate accuracy\n",
    "        if np.argmax(ytrain[nn_index]) == np.argmax(ytest[i]):\n",
    "            accuracy += 1./len(xtestNorm)\n",
    "    print(\"Accuracy : \",accuracy)\n",
    "    \n",
    "    # Convert the model.\n",
    "    converter = tf.lite.TFLiteConverter.from_session(sess, [xtr], [out])\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the model.\n",
    "    with open('model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. TESTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "print(\"<<MODELS REPORT LOG. REGRESSION>>\")\n",
    "print(\"Logistic Regression score:\",logReg.score(xtestNorm, ytest))\n",
    "\n",
    "predictions = logReg.predict(xtestNorm)\n",
    "print(\"Logistic Regression Model accuracy:\", metrics.accuracy_score(ytest, predictions))\n",
    "print(metrics.classification_report(ytest,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "print(\"<<MODEL REPORT KNN>>\")\n",
    "print(\"KNN score:\",knn.score(xtestNorm, ytest))\n",
    "\n",
    "predictions = knn.predict(xtestNorm)\n",
    "print(\"KNN Model accuracy:\", metrics.accuracy_score(ytest, predictions))\n",
    "print(metrics.classification_report(ytest,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_evaluation_dataframe(model, xdata, ydata, classes_dict):\n",
    "    real = []; predicted = []; accuracy = []\n",
    "    \n",
    "    classes, indices, counts = np.unique(ydata, return_counts=True, return_index=True)\n",
    "    \n",
    "    well_predicted = {key:0 for key in classes}\n",
    "    #well_predicted.fromkeys(classes, 0)\n",
    "    \n",
    "    probabilities = model.predict_proba(x)\n",
    "    \n",
    "    for (features, lbl, probs) in zip(xdata, ydata, probabilities):\n",
    "        r = int(lbl)\n",
    "        real.append(classes_dict[r])\n",
    "        p = int(model.predict([features]))\n",
    "        predicted.append(classes_dict[p])\n",
    "        accuracy.append(max(probs))\n",
    "        if(r == p):\n",
    "            well_predicted[r] += 1\n",
    "    \n",
    "    list_of_tuples = list(zip(real, predicted, accuracy))\n",
    "    \n",
    "    i = 0\n",
    "    for cl in classes:\n",
    "        idx = int(cl)\n",
    "        print(classes_dict[idx], well_predicted[idx], counts[i])\n",
    "        i+=1\n",
    "    \n",
    "    df = pd.DataFrame(list_of_tuples, columns = ['Real class', 'Predicted class', 'Accuracy'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_dataframe(logReg, xval, yval, classes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_dataframe(knn, xval, yval, classes_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PI",
   "language": "python",
   "name": "pi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
